const fs = require("fs");
const path = require("path");
const mongoose = require("mongoose");
const natural = require("natural");
const stopwords = require("stopword");
const Book = require("../models/Book");
require("dotenv").config();

mongoose.connect(process.env.MONGODB_URI, {
  useNewUrlParser: true,
  useUnifiedTopology: true,
});

const tokenizer = new natural.WordTokenizer();
const lemmatizer = new natural.WordNetLemmatizer(); // âœ… Ajout de la lemmatisation
const indexCollection = mongoose.connection.collection("index");

// âœ… Filtrage strict des mots
const isValidWord = (word) => /^[a-zA-Z]{4,}$/.test(word); // Exclut nombres, symboles, et mots courts

async function indexBooks() {
  console.log("ğŸ” DÃ©but de l'indexation...");
  const { deletedCount } = await indexCollection.deleteMany({});
  console.log(`ğŸ—‘ï¸ Suppression des anciens index : ${deletedCount} documents supprimÃ©s`);

  const books = await Book.find({});
  console.log(`ğŸ“š Nombre de livres trouvÃ©s : ${books.length}`);

  let batchUpdates = [];
  let wordIndex = {};

  for (let book of books) {
    try {
      const filePath = path.join(__dirname, "..", book.contentPath);
      if (!fs.existsSync(filePath)) {
        console.error(`âŒ Fichier introuvable: ${filePath}`);
        continue;
      }

      const stream = fs.createReadStream(filePath, { encoding: "utf8" });

      let wordCounts = {};

      for await (const chunk of stream) {
        let words = tokenizer.tokenize(chunk.toLowerCase());
        words = stopwords.removeStopwords(words); // âœ… Suppression des stopwords
        words = words.filter(isValidWord).map(lemmatizer.lemmatize); // âœ… Filtrage + Lemmatisation

        words.forEach((word) => {
          if (!wordCounts[word]) wordCounts[word] = 0;
          wordCounts[word] += 1;
        });
      }

      // âœ… Stocker les mots-clÃ©s en mÃ©moire pour Ã©viter les ralentissements
      for (const [word, count] of Object.entries(wordCounts)) {
        if (!wordIndex[word]) wordIndex[word] = {};
        wordIndex[word][book._id] = count;
      }

      console.log(`âœ… Livre indexÃ© : ${book.title} (${Object.keys(wordCounts).length} mots uniques)`);

      // âœ… InsÃ©rer en bulk aprÃ¨s 5000 mots stockÃ©s en mÃ©moire
      if (Object.keys(wordIndex).length > 5000) {
        await insertBatch(wordIndex);
        wordIndex = {};
      }

    } catch (err) {
      console.error(`âŒ Erreur sur ${book.title}:`, err);
    }
  }

  // âœ… Dernier batch Ã  insÃ©rer
  if (Object.keys(wordIndex).length > 0) {
    await insertBatch(wordIndex);
  }

  console.log(`âœ… Indexation terminÃ©e ! ${await indexCollection.countDocuments()} mots indexÃ©s.`);
  mongoose.connection.close();
}

// âœ… Insertion optimisÃ©e en MongoDB avec `bulkWrite`
async function insertBatch(wordIndex) {
  const bulkOps = Object.entries(wordIndex).map(([word, books]) => ({
    updateOne: {
      filter: { word },
      update: { $set: { books } },
      upsert: true, // âœ… Ajoute si Ã§a n'existe pas
    },
  }));

  await indexCollection.bulkWrite(bulkOps);
  console.log(`ğŸ“¤ Batch de ${bulkOps.length} mots insÃ©rÃ© dans MongoDB !`);
}

// ğŸš€ Lancer l'indexation
// indexBooks().catch((err) => console.error("âŒ Erreur:", err));
